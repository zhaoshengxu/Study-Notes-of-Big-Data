# 大数据初步学习
## 一、大数据突然变得热门的原因：
> 大数据分界点大概是在2010/2011年，主要是这两年发生了一下三个变化：
1. 数据采集门槛大大降低，**数据体量变大**
2. 以卷积神经网络(CNN)为代表的**算法**
3. **计算机的计算能力的提升**，CUDA的GPU并行计算


## 二、大数据及相关研究领域
> 所谓**大数据**就是**从海量数据中提取信息**的科学。

**大数据相关研究领域**（不含硬件）：

1. 数据准备（data preparation）
2. 数据清洁（data clean）
3. 数据统计（statistics）
4. 数据挖掘（data mining）
5. 机器学习（machine learning）
6. 深度学习（deep learning） 
7. 计算机视觉（computer vision）
8. 三维建模（3D modeling）等等

## 三、机器学习与深度学习

> - **机器学习（machine learning）** 就是传统的机器学习，包括普通的神经网络（neural network）、支持向量机（support vector machine）等
> - **深度学习（deep learning）** 是以卷积神经网络（CNN）为代表的新型的机器学习（machine learning）

### 1. **深度学习**

无论是机器学习，还是深度学习，都是机器在人的指导下学习的过程，而学的就是数学中讲的一一映射。所谓大数据学习就是用大量数据喂养机器，教他学习认识事物的过程。

- **优点**：极强的计算能力，暴力学习，不眠不休。
- **缺点**：被动学习，缺少逻辑推理能力

目前，虽然机器学习很暴力，但是还远没有发展到一个极致的程度。现在的深度学习其实就是模仿一个儿童学习的过程。儿童不仅具有认知能力，还有在认识之后，联想、对比的逻辑推理能力，可是这个逻辑推理能力，目前还没有办法将其数学化，并教给机器。

比如，面部识别，可能能够识别一个人年轻时的样子，但是如果再给他一张老年时的照片，他将无法识别出老年的样子。

### 2. **机器学习与深度学习准确度的差别**

![deep learning and machine learning](http://oez33r3ch.bkt.clouddn.com/deep%20learning)

由上图可以看出：

- 随着数据量的增多，机器学习的准确度会达到一个平台，基本不再变化。
- 而**深度学习的准确度则是随着数据量的增多而逐渐升高**，这正是我们所希望的样子。


### 3. 大数据，不能只看到“大”，而忽略了“数据”

> 数据很重要，数据要尽可能干净，尽可能好，在学习时，合理安排数据也非常重要


#### 3.1 问题

> 一组数据，如果900个普通数据A，和100个稀有数据B，深度学习时，效果非常好，但是预测时，预测A类普通数据效果很好，而预测B类稀有数据，则效果很糟糕。该如何处理？


#### 3.2 简单理解

> 对于一个机器来说，如果这个机器是个懒蛋，对我来说你不就让我预测的准嘛。既然你是900个A和100个B，那么我不管是A还是B，我都说是A，因为总共1000个数据，就算100个 B全错了，准确率也是90%。

之所以对于少量的B类数据预测效果很糟糕，是因为你给的数据就存在一个巨大的偏差。

事实可能确实是，A类普通数据容易获取，所以有很多数据；而B类数据可能本身出现的频率比较低，所以比较少。

可是不管B类数据怎么少，如果你给的数据量是（900A/100B）这样一个比例关系的话。那么机器学习的过程中，他就有一个这种有一个这种趋势：尽量满足A而不满足B，因为满足A可以得到很高的准确度，准确度较低的就被忽略了。而实际上，机器并没有忽略100个B，他训练的也挺好，可是当新的B类数据传入时，机器还是选了忽略。

这件事情很值得去思考，机器其实并不傻，你训练机器时，不断地教他学习，他不停的计算，你给他100个B类数据，他就乖乖记住，但是新来的数据他就不记了，有点像是能量最小化、路径依赖，总之，怎么简单怎么来。

### 4. 一句话

> **知之为知之，不知Google知**